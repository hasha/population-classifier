{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Here I create a populations of neurons (size=80, 40 of which respond to stimulus 0 and 40 to stimulus 1).In the \"homogenous\"\n",
    "#case the spikes are Poisson and the average number of spikes is uniform across the trial length. The only difference \n",
    "#between the response of neurons tuned to stim 0 vs. stim 1 is the Poisson rate parameter (80 vs. 100). In the \"nonhomogenous\" case\n",
    "#what differentiates the two types of neurons is not their average firing rate (which is the same), but rather the difference\n",
    "#\n",
    "\n",
    "n_neurons = 80 #per condition\n",
    "fr_stim = 60\n",
    "dt = float(1)/1000\n",
    "length_trial = 100 #in ms\n",
    "num_trials = 400\n",
    "#in the end we combine stim 1 and stim 2 so in total there will be 800 trials\n",
    "#train on 600, test on 200\n",
    "\n",
    "def is_odd(num):\n",
    "    return bool(num % 2 != 0)\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def interleave(mat1,mat2,num_rows):\n",
    "    unit_row = np.shape(mat1)[0]/num_rows;\n",
    "    unit_col = np.shape(mat1)[1]\n",
    "    interleaved = np.empty((0,unit_col))\n",
    "    if (np.shape(mat1)[0]%num_rows)>0:\n",
    "        print \"Error\"\n",
    "        return\n",
    "    else:\n",
    "        start = 0\n",
    "        for rows in range(unit_row):\n",
    "            end = start + num_rows\n",
    "            combined = np.append(mat1[start:end,:],mat2[start:end,:],axis=0)\n",
    "            interleaved = np.concatenate((interleaved,combined),axis=0)\n",
    "            start = start + num_rows\n",
    "    return interleaved\n",
    "\n",
    "def poissonGroup(fr,length_trial,n_neurons,dt,num_trials,type_):\n",
    "    neurons_by_trials = n_neurons*(num_trials/2)\n",
    "    if type_ == 1:\n",
    "        truth_mat = np.random.uniform(0,1,(neurons_by_trials,length_trial)) < (fr*dt)\n",
    "        fr_null = 0.9 * fr\n",
    "        truth_mat2 = np.random.uniform(0,1,(neurons_by_trials,length_trial)) < (fr_null*dt)\n",
    "    else:\n",
    "        truth_mat2 = np.random.uniform(0,1,(neurons_by_trials,length_trial)) < (fr*dt)\n",
    "        fr_null = 0.9 * fr\n",
    "        truth_mat = np.random.uniform(0,1,(neurons_by_trials,length_trial)) < (fr_null*dt)\n",
    "    poisson = interleave(truth_mat, truth_mat2,(n_neurons/2))\n",
    "    return poisson\n",
    "\n",
    "#homogenous case\n",
    "#type_ refers to the subset of neurons that are most driven by the input\n",
    "#1- first half, 2- second half\n",
    "pop_stim1 = 1*poissonGroup(fr_stim,length_trial,n_neurons,dt,num_trials,1)\n",
    "pop_stim2 = 1*poissonGroup(fr_stim,length_trial,n_neurons,dt,num_trials,2)\n",
    "\n",
    "mean_stim1 = np.mean(np.sum(pop_stim1[:40],axis=1))\n",
    "mean_stim2 = np.mean(np.sum(pop_stim1[40:80,:],axis=1))\n",
    "\n",
    "#Non-homogenous case (rate changes every 10 ms, anti-correlated neurons)\n",
    "step_time = length_trial/10\n",
    "pop_corr1 = np.empty((n_neurons*num_trials,0)) #pre-allocate matrices\n",
    "pop_corr2 = np.empty((n_neurons*num_trials,0))\n",
    "for t in range(step_time):\n",
    "    if is_odd(t):\n",
    "        spikes1 = 1*poissonGroup(fr_stim,step_time,n_neurons,dt,num_trials,1)\n",
    "        spikes2 = 1*poissonGroup(fr_stim,step_time,n_neurons,dt,num_trials,2)\n",
    "    else:\n",
    "        spikes1 = 1*poissonGroup(fr_stim,step_time,n_neurons,dt,num_trials,2)\n",
    "        spikes2 = 1*poissonGroup(fr_stim,step_time,n_neurons,dt,num_trials,1) \n",
    "    pop_corr1 = np.concatenate((pop_corr1,spikes1),axis=1)\n",
    "    pop_corr2 = np.concatenate((pop_corr2,spikes2),axis=1)\n",
    "    \n",
    "\n",
    "#Create a function to convert into 3-dimensional matrices of the form:\n",
    "#[n_neurons,n_length,n_trials]\n",
    "#n_neurons = n_neurons*2\n",
    "def reshape(mat,n_neurons,length_trial,num_trials):\n",
    "    reshaped = np.empty((n_neurons,length_trial,num_trials))\n",
    "    for trial in range(num_trials):\n",
    "        idx_end = (trial+1) * n_neurons\n",
    "        idx_start = idx_end - n_neurons\n",
    "        reshaped[:,:,trial] = mat[idx_start:idx_end,:]\n",
    "    return reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For convenience we store all our data in 3D matrix format\n",
    "\n",
    "#Model 1:\n",
    "pop_stim1_3d = reshape(pop_stim1,n_neurons,length_trial,num_trials)\n",
    "pop_stim2_3d = reshape(pop_stim2,n_neurons,length_trial,num_trials)\n",
    "\n",
    "#Model2:\n",
    "pop_corr1_3d = reshape(pop_corr1,n_neurons,length_trial,num_trials)\n",
    "pop_corr2_3d = reshape(pop_corr2,n_neurons,length_trial,num_trials)\n",
    "\n",
    "del pop_stim1, pop_stim2, pop_corr1, pop_corr2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This section visually validates that the method we used to generate the spikes worked properly\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pref_stim1 = pop_stim1_3d[0:40,:,:]\n",
    "null_stim1 = pop_stim1_3d[40:80,:,:]\n",
    "\n",
    "pref_sum1 = np.sum(np.sum(pref_stim1,axis=2),axis=0)\n",
    "null_sum1 = np.sum(np.sum(null_stim1,axis=2),axis=0)\n",
    "\n",
    "pref_corr1 = pop_corr1_3d[0:40,:,:]\n",
    "null_corr1 = pop_corr1_3d[40:80,:,:]\n",
    "pref_sum1_corr = np.sum(np.sum(pref_corr1,axis=2),axis=0)\n",
    "null_sum1_corr = np.sum(np.sum(null_corr1,axis=2),axis=0)\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('2 models of encoding - Stimulus 1')\n",
    "plt.plot((pref_sum1/200)*10)\n",
    "plt.hold(True)\n",
    "plt.plot((null_sum1/200)*10)\n",
    "plt.ylim(30, 55)\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot((pref_sum1_corr/200)*10)\n",
    "plt.hold(True)\n",
    "plt.plot((null_sum1_corr/200)*10)\n",
    "plt.ylim(30, 55)\n",
    "\n",
    "# These plots demonstrate two different encoding models. We hypothesize that logistic regression on rates will perform\n",
    "#well on the first but not the second model\n",
    "#The second model consists of a latent variable (preferred activity in each neuronal pool fluctuates and drives the \n",
    "# activity of the other population down). This is of course a cartoon model but serves to validate our method.\n",
    "# blue is the preferred pool and green is the null. \n",
    "# For stimulus 2 these are just flipped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "\n",
    "scaler = StandardScaler()  \n",
    "clf = linear_model.LogisticRegression()\n",
    "#clf = MLPClassifier(solver='lbfgs', alpha=1e-5,hidden_layer_sizes=(5, 2), random_state=1\n",
    "\n",
    "def rate_feature_extraction(mat_3d,stim_type):\n",
    "    all_data = np.empty((0,n_neurons+1))\n",
    "    for trial in range(num_trials):\n",
    "        trial_array = np.append(np.sum(mat_3d[:,:,trial],axis=1),stim_type)\n",
    "        all_data = np.concatenate((all_data,[trial_array]),axis=0)\n",
    "    return all_data\n",
    "\n",
    "def test_accuracy(mat1,mat2,clf):\n",
    "    all_ = np.concatenate((mat1,mat2),axis=0)\n",
    "    X_all = all_[:,0:-1]\n",
    "    y_all = all_[:,-1]\n",
    "    #Shuffle and split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all, train_size=600)\n",
    "    scaler.fit(X_train)  \n",
    "    X_train = scaler.transform(X_train)  \n",
    "    # apply same transformation to test data\n",
    "    X_test = scaler.transform(X_test) \n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = float(np.sum(1*(y_pred==y_test)))/(np.shape(X_test)[0])\n",
    "    return accuracy, clf, X_test, y_pred\n",
    "\n",
    "#Model1:\n",
    "#Independent rate model\n",
    "#Apply a logistic regression to the spike counts\n",
    "#Here each neuron's firing rate is a feature\n",
    "\n",
    "#columns are spike counts, last column is stimulus type, rows are trials\n",
    "rate_stim1 = rate_feature_extraction(pop_stim1_3d,0)\n",
    "rate_stim2 = rate_feature_extraction(pop_stim2_3d,1)\n",
    "\n",
    "accuracy1, clf1, X_test1, y_pred1 = test_accuracy(rate_stim1,rate_stim2,clf)\n",
    "class1 = clf.coef_[:,0:20]\n",
    "class2 = clf.coef_[:,20:40]\n",
    "\n",
    "sigmoid_pred = 1*(sigmoid(X_test1.dot(clf1.coef_.T) + clf1.intercept_.T) > 0.5)\n",
    "sigmoid_pred = np.ravel(sigmoid_pred)\n",
    "similarity = np.sum(1*(y_pred1==sigmoid_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Correlation code model\n",
    "# Here the average firing rates are the same but the pattern of firing is different for the subsets of the population\n",
    "# However the pattern is such that using spike counts over the 100ms window does not capture the difference\n",
    "\n",
    "clf = linear_model.LogisticRegression()\n",
    "#empty clf\n",
    "\n",
    "rate_stim1 = rate_feature_extraction(pop_corr1_3d,0)\n",
    "rate_stim2 = rate_feature_extraction(pop_corr2_3d,1)\n",
    "\n",
    "accuracy2, clf2, X_test2, y_pred2  = test_accuracy(rate_stim1,rate_stim2,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def spike_feature_extraction(mat_3d,stim_type):\n",
    "    all_data = np.empty((0,(n_neurons*length_trial)+1))\n",
    "    for trial in range(num_trials):\n",
    "        trial_array = np.append(mat_3d[:,:,trial].ravel(),stim_type)\n",
    "        all_data = np.concatenate((all_data,[trial_array]),axis=0)\n",
    "    return all_data\n",
    "\n",
    "spike_stim1 = spike_feature_extraction(pop_stim1_3d,0)\n",
    "spike_stim2 = spike_feature_extraction(pop_stim2_3d,1)\n",
    "accuracy3, clf3, X_test3, y_pred3 = test_accuracy(spike_stim1,spike_stim2,clf)\n",
    "\n",
    "\n",
    "spike_corr1 = spike_feature_extraction(pop_corr1_3d,0)\n",
    "spike_corr2 = spike_feature_extraction(pop_corr2_3d,1)\n",
    "accuracy4, clf4, X_test4, y_pred4 = test_accuracy(spike_corr1,spike_corr2,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot([accuracy1,accuracy2,accuracy3,accuracy4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Model 2 with restricted boltzmann machine\n",
    "from sklearn.neural_network import BernoulliRBM\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def test_accuracy_rbm(mat1,mat2):\n",
    "    logistic = linear_model.LogisticRegression()\n",
    "    rbm = BernoulliRBM(random_state=0, verbose=True, n_components = 200, n_iter = 30)\n",
    "    clf_rbm = Pipeline(steps=[('rbm', rbm), ('logistic', logistic)])\n",
    "    all_ = np.concatenate((mat1,mat2),axis=0)\n",
    "    X_all = all_[:,0:-1]\n",
    "    y_all = all_[:,-1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_all, y_all,train_size=600,random_state=0)\n",
    "    clf_rbm.fit(X_train,y_train)\n",
    "    y_pred = clf_rbm.predict(X_test)\n",
    "    accuracy = float(np.sum(1*(y_pred==y_test)))/(np.shape(X_test)[0])\n",
    "    return accuracy\n",
    "\n",
    "accuracy = test_accuracy_rbm(spike_corr1,spike_corr2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Models we will use"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
